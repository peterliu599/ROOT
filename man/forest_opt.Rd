% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/core_functions.R
\name{forest_opt}
\alias{forest_opt}
\title{Ensemble of weighted trees (forest) and Rashomon set selection}
\usage{
forest_opt(
  data,
  outcome,
  treatment,
  sample,
  leaf_proba = 0.25,
  seed = NULL,
  num_trees = 10,
  vote_threshold = 2/3,
  explore_proba = 0.05,
  feature_est = "Ridge",
  top_k_trees = FALSE,
  k = 10,
  cutoff = "baseline",
  verbose = FALSE
)
}
\arguments{
\item{data}{A data frame containing the dataset (must include outcome, treatment, sample indicator).}

\item{outcome}{Name of the outcome column.}

\item{treatment}{Name of the treatment indicator column (0/1).}

\item{sample}{Name of the sample indicator column (0/1).}

\item{leaf_proba}{Probability mass for the "leaf" option in each tree (see \code{\link{tree_opt}}; default 0.25).}

\item{seed}{Integer seed for reproducibility (default 42).}

\item{num_trees}{Number of trees to grow in the forest (default 10).}

\item{vote_threshold}{Majority vote threshold in (0.5, 1] for assigning final weight=1 (default 2/3, i.e., at least 67\% of trees vote 1).}

\item{explore_proba}{Probability of exploration at leaves in each tree (default 0.05, see \code{\link{split_node}}).}

\item{feature_est}{Method for feature importance estimation, \code{"Ridge"} (default) or \code{"GBM"}.}

\item{top_k_trees}{Logical. If \code{TRUE}, selects the Rashomon set as the top \code{k} trees with lowest objective. If \code{FALSE}, uses a cutoff threshold.}

\item{k}{Integer, number of top trees to keep if \code{top_k_trees = TRUE} (default 10).}

\item{cutoff}{Either \code{"baseline"} or a numeric value. If \code{top_k_trees = FALSE}, this is the loss cutoff for selecting Rashomon set (if \code{"baseline"}, uses the baseline loss of not weighting any observations).}

\item{verbose}{Logical, if \code{TRUE}, prints progress messages (e.g., estimated PATE and number of trees selected). Default \code{FALSE}.}
}
\value{
A list with components:
\item{D_rash}{A data frame similar to \code{D_forest} but containing only the Rashomon set of trees' weight columns and additional columns: \code{w_opt} (the ensemble-voted weight for each observation) and \code{vote_count} (how many trees voted to include that observation).}
\item{D_forest}{A data frame with all trees' weight assignments. It contains covariates, \code{v}, \code{vsq}, \code{S}, \code{lX} (overlap factor = 1/b), and columns \verb{w_tree_1, ..., w_tree_num_trees} for each tree's weights.}
\item{w_forest}{A list of length \code{num_trees}, where each element is the full tree object returned by the internal split routine (containing the tree structure and local objective).}
\item{rashomon_set}{An integer vector of indices of trees that were selected into the Rashomon set.}
\item{f}{An \code{rpart} model fit on \code{X} vs \code{w_opt} (the final classifier summarizing the ensemble's selected subgroup).}
\item{testing_data}{The data frame of observations used in tree construction (same as in \code{tree_opt} and output from \code{estimate_dml}, i.e., those with \code{S==1} and finite pseudo-outcomes).}
}
\description{
Builds multiple weighted trees via \code{\link{tree_opt}} logic, then identifies a "Rashomon set" of top-performing trees and aggregates their weight assignments by majority vote.
}
\details{
Each tree in the forest is built on the same cross-fitted pseudo-outcomes (from one call to \code{\link{estimate_dml}}) for consistency. If \code{feature_est = "GBM"}, a gradient boosting model (from \pkg{gbm}) is used instead of ridge regression to estimate feature importance for splitting probabilities. The Rashomon set is defined as either the top \code{k} trees with lowest objective (if \code{top_k_trees=TRUE}), or all trees with objective below a cutoff. The baseline loss is defined as the loss with no selection (all weights = 1 for S==1 group).
}
\examples{
 sim<-get_data(n=2000,seed=599)
 D <-sim$data
 dml<-estimate_dml(D,outcome="Yobs",treatment="Tr",sample="S",crossfit= 5)
 fo_res<-forest_opt(D,
                  outcome="Yobs",treatment="Tr",sample="S",
                  leaf_proba=0.25,seed=3,
                  num_trees=50, vote_threshold=2/3,
                  explore_proba=0.05,feature_est="Ridge",
                  top_k_trees=FALSE,cutoff="baseline")
}
