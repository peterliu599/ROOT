% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/core_functions.R
\name{split_node}
\alias{split_node}
\title{Recursive split builder for weighted tree (internal function)}
\usage{
split_node(
  split_feature,
  X,
  D,
  parent_loss,
  depth,
  explore_proba = 0.05,
  choose_feature = choose_feature,
  loss_fn = loss,
  reduce_weight_fn = reduce_weight,
  max_depth = 8,
  min_leaf_n = 5,
  log_fn = function(...) {
 }
)
}
\arguments{
\item{split_feature}{Named numeric vector of feature selection probabilities (should include a "leaf" option).}

\item{X}{A data frame of current observations (must include at least the feature \code{fj} chosen for splitting if applicable; may also include a working copy of weights \code{w}).}

\item{D}{A data frame representing the global state (must include columns \code{w} for weights and \code{vsq} for squared outcomes, with row names aligning to observations).}

\item{parent_loss}{Numeric, the loss value of the parent node (used to decide if a split improves the objective).}

\item{depth}{Integer, current depth of the node in the tree.}

\item{explore_proba}{Numeric in \verb{[0,1]}, probability of exploring the suboptimal split at a leaf (randomly flipping the chosen weight).}

\item{choose_feature}{Function to choose the next feature to split (default uses \code{choose_feature}).}

\item{loss_fn}{Function to compute loss given a tentative weight assignment (default uses \code{loss}).}

\item{reduce_weight_fn}{Function to adjust feature weights when a split is rejected (default uses \code{reduce_weight}).}

\item{max_depth}{Integer maximum depth of the tree. If the current depth equals \code{max_depth}, the node is made a leaf.}

\item{min_leaf_n}{Integer minimum number of observations required to attempt a split. If \code{X} has <= \code{min_leaf_n} rows, the node becomes a leaf.}

\item{log_fn}{Function for logging debug messages. By default, a no-op. If provided (e.g., from \code{tree_opt(verbose=TRUE)}), it will receive formatted strings to output.}
}
\value{
A list representing the tree/subtree. Each node includes:
\item{node}{Feature name used for split at this node (or "leaf").}
\item{split}{Numeric value of the splitting threshold (midpoint) if node is a split.}
\item{left_tree}{Left subtree (list) if node is a split.}
\item{right_tree}{Right subtree (list) if node is a split.}
\item{w}{If node is a leaf, the weight (0 or 1) assigned to all observations in this node.}
\item{local objective}{Numeric, the loss objective value at this node after assignment.}
\item{depth}{Integer depth of this node.}
\item{D}{Data frame of global state after this node's assignments (for internal use).}
\item{leaf_reason}{(Leaf nodes only) Text reason for why this node was made a leaf (e.g., "max-depth", "min-leaf", "feature==leaf", etc.).}
\item{feature}{(Leaf nodes only) The feature that would have been split (or NA).}
\item{cut}{(Leaf nodes only) The cut value (or NA) that would have been used.}
}
\description{
Recursively builds a weighted decision tree to optimize an objective function, using exploration/exploitation trade-off. This function is internal and is used by \code{tree_opt()} to construct a single tree.
}
\details{
This function works as follows:
\enumerate{
\item Selects a feature \code{fj} to split using \code{choose_feature} and the current probability vector \code{split_feature}. If \code{fj == "leaf"}, no further splitting is done and the node becomes a leaf.
\item If the maximum depth or minimum node size criteria are met, the node becomes a leaf.
\item Otherwise, it computes a split threshold (\code{cj}) as the midpoint of feature \code{fj} in the current data \code{X}, and partitions \code{X} (and the corresponding indices in \code{D}) into left (\code{X_left}) and right (\code{X_right}) sets.
\item If either side is empty, the node becomes a leaf (no split possible).
\item It then evaluates the loss for assigning all left observations weight 0 vs 1, and all right observations weight 0 vs 1, to determine optimal assignments (\code{w_left} and \code{w_right} for each side).
\item If the combined loss \code{new_loss} is less than or equal to the \code{parent_loss}, the split is accepted: weights in \code{D} are updated for left and right, and the function recurses on each side (randomizing the order of recursion to break symmetry).
\item If the split does not improve the loss, it is rejected: the selection probability of \code{fj} is reduced (via \code{reduce_weight_fn}), and \code{split_node} is called again on the same data without increasing depth.
}
Leaf node assignment: when a node is designated as a leaf, it compares the loss of assigning all observations in that node \code{w=0} versus \code{w=1} and picks the assignment that yields lower loss (exploitation). With probability \code{explore_proba}, it may instead pick the opposite assignment to encourage exploration. This randomness can be controlled via the \code{explore_proba} parameter.
}
\note{
This function is typically not called directly by the user. It assumes that \code{D} and \code{X} are kept in sync and that \code{D$w} is updated globally as splits are made. The \code{log_fn} parameter allows optional logging of the splitting process for debugging or insight when \code{verbose=TRUE} is passed from higher-level functions.
}
