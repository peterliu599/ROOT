% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ROOT.R
\name{ROOT}
\alias{ROOT}
\title{Rashomon Set of Optimal Trees (ROOT) for Functional Optimization}
\usage{
ROOT(
  data,
  global_objective_fn = NULL,
  generalizability_path = FALSE,
  leaf_proba = 0.25,
  seed = NULL,
  num_trees = 10,
  vote_threshold = 2/3,
  explore_proba = 0.05,
  feature_est = "Ridge",
  feature_est_args = list(),
  top_k_trees = FALSE,
  k = 10,
  cutoff = "baseline",
  max_depth = 8L,
  min_leaf_n = 2L,
  max_rejects_per_node = 10L,
  verbose = FALSE
)
}
\arguments{
\item{data}{A data.frame containing the dataset.

In \strong{general optimization mode} (\code{generalizability_path =
  FALSE}), \code{data} can contain any covariates and auxiliary columns.
The user supplies a \code{global_objective_fn} that takes a data frame
with a column \code{w} and returns a scalar loss.

In \strong{generalizability mode} (\code{generalizability_path = TRUE}),
\code{data} must contain columns \code{"Y"} (outcome), \code{"Tr"}
(treatment indicator, 0/1), and \code{"S"} (sample indicator, 1 = trial,
0 = target). ROOT internally constructs transportability scores and, if
no custom objective is given, uses a default variance-based loss.}

\item{global_objective_fn}{A function with signature
\code{function(D) -> numeric} scoring the entire state and minimized by
ROOT. \code{D} is the working data frame containing at least a column
\code{w} with the current weight assignments. If \code{NULL} (default), a
variance-based objective is used.}

\item{generalizability_path}{\code{Logical(1)}. If \code{TRUE}, use the
built-in transportability objective for trial-to-target generalizability.
If \code{FALSE}, treat \code{data} as arbitrary and rely on
\code{global_objective_fn}. Default \code{FALSE}.}

\item{leaf_proba}{A numeric tuning parameter that increases the chance
a node stops splitting by selecting a synthetic \code{"leaf"} feature.
Internally, the probability of choosing \code{"leaf"} is
\code{leaf_proba / (1 + leaf_proba)} (assuming the
covariate probabilities sum to 1). Higher values produce shallower,
sparser trees. Default \code{0.25}.}

\item{seed}{An optional numeric seed for reproducibility.}

\item{num_trees}{An integer number of trees to grow. More trees explore the
solution space more thoroughly. Default 10.}

\item{vote_threshold}{Controls how per-observation votes from the Rashomon
set trees are aggregated into the final binary weight \code{w_opt}.
Accepts one of:
\itemize{
\item A \strong{numeric} in \code{(0, 1)}: \code{w_opt = 1} when the
mean fraction of Rashomon-set trees voting 1 strictly exceeds this
value. An error is raised if any observation's mean vote equals the
threshold exactly (tie is undefined). Default \code{2/3}.
\item \code{"majority"}: equivalent to the numeric threshold \code{2/3}.
\item \code{"mean"}: \code{w_opt = 1} when the mean vote strictly
exceeds \code{0.5}. Errors on exact \code{0.5} ties.
\item \code{"median"}: \code{w_opt = 1} when the per-row median vote
strictly exceeds \code{0.5}. Errors on exact \code{0.5} ties.
\item A \strong{function} with signature
\code{function(votes) -> integer vector}, where \code{votes} is a
numeric matrix with one row per observation and one column per
Rashomon-set tree (each cell 0 or 1). Must return an integer vector
of 0s and 1s of length \code{nrow(votes)}.
}}

\item{explore_proba}{A numeric giving the exploration probability at
leaves. With probability \code{explore_proba}, a leaf receives a random
weight; otherwise the greedy-optimal weight is used. Default \code{0.05}.}

\item{feature_est}{Either \code{"Ridge"}, \code{"GBM"}, or a custom
\code{function(X, y, ...) -> named numeric vector} returning nonnegative
importance values with names matching columns of \code{X}. Used to bias
which covariates are chosen for splitting. If the method fails, ROOT
falls back to uniform feature sampling with a warning.}

\item{feature_est_args}{A list of additional arguments passed to a
user-supplied \code{feature_est} function.}

\item{top_k_trees}{\code{Logical(1)}. If \code{TRUE}, select the top
\code{k} trees by objective value for the Rashomon set. If \code{FALSE},
use the \code{cutoff} threshold. Default \code{FALSE}.}

\item{k}{An integer giving the number of top trees when
\code{top_k_trees = TRUE}. Default 10.}

\item{cutoff}{A numeric or \code{"baseline"}. Used as the Rashomon cutoff
when \code{top_k_trees = FALSE}. \code{"baseline"} uses the objective at
\eqn{w \equiv 1} (all weights equal to 1, i.e., no exclusions).}

\item{max_depth}{Maximum depth of each tree grown during the forest
construction stage. A node at \code{depth == max_depth} is forced to be a
leaf. Shallower trees are more interpretable but less flexible. Default
\code{8}.}

\item{min_leaf_n}{Minimum number of observations required in a node for
splitting to be attempted. If a node contains fewer than
\code{min_leaf_n} observations it becomes a leaf. Default \code{2}.}

\item{max_rejects_per_node}{Maximum number of consecutive rejected splits
(splits that do not improve the objective) allowed at a single node
before the node is forced to become a leaf. This prevents infinite
recursion in pathological cases. Default \code{10}.}

\item{verbose}{\code{Logical(1)}. If \code{TRUE}, prints the unweighted and
(when available) weighted treatment effect estimates and standard errors
in generalizability mode)}
}
\value{
An object of class \code{"ROOT"} (a list) with elements:
\item{D_rash}{Data frame containing the Rashomon-set tree votes and the
final aggregated weight \code{w_opt} for each observation.}
\item{D_forest}{Data frame with all forest-level working columns.}
\item{w_forest}{List of per-tree results from the tree-building routine.}
\item{rashomon_set}{Integer vector of indices identifying which trees
were selected into the Rashomon set.}
\item{global_objective_fn}{The objective function used.}
\item{f}{The characteristic (summary) tree fitted to \code{w_opt}, as an
\code{rpart} object. \code{NULL} if all observations received the same
weight or no covariates were available.}
\item{testing_data}{Data frame of observations used for optimization
(trial units when \code{generalizability_path = TRUE}).}
\item{estimate}{(Only when \code{generalizability_path = TRUE}) A list
with the unweighted (SATE) and weighted (WTATE) point estimates, and standard errors}
\item{generalizability_path}{Logical flag echoing the input.}
}
\description{
ROOT (Rashomon Set of Optimal Trees) is a general-purpose functional
optimization algorithm that learns interpretable, tree-structured binary
weight functions \eqn{w(X) \in \{0, 1\}}. Given a dataset \eqn{D_n} and a
global objective function \eqn{L(w, D_n)}, ROOT searches over the space of
decision trees to find weight assignments that minimize the objective function.
}
\section{The optimization problem}{

ROOT solves the functional optimization problem:

\deqn{w^* \in \arg\min_w L(D_n, w)}

where \eqn{w: \mathbb{R}^p \to \{0, 1\}} maps a \eqn{p}-dimensional
covariate vector to a binary include/exclude decision. The key challenge is
that, unlike standard tree algorithms, the global loss
\eqn{L(D_n, w)} is not decomposable as a sum of losses over
independent subsets of the data. This means conventional greedy,
divide-and-conquer tree-building strategies do not apply. ROOT addresses
this through a randomization-based tree construction with an
explore-exploit strategy.
}

\section{How ROOT works}{

The algorithm proceeds in several stages:

\enumerate{
\item \strong{Feature importance estimation:} Split probabilities are
estimated using Ridge regression, Gradiant Boosting Machine (GBM),
or a user-supplied function,biasing the search toward covariates likely
to be informative.
\item \strong{Stochastic tree construction:} \code{num_trees} trees are
grown. At each internal node, a feature is drawn according to the
estimated split probabilities (or a "leaf" token is drawn, terminating
the branch). Splits are made at the midpoint of the selected feature's
empirical distribution. An explore-exploit strategy assigns leaf
weights: with probability \code{explore_proba} a random weight is
chosen; otherwise the greedy optimal weight (reducing the global
objective) is used.
\item \strong{Rashomon set selection:} Trees are ranked by their global
objective values. The top-\code{k} trees (or all trees below a cutoff)
form the Rashomon set: a collection of near-optimal but
potentially different models, each providing a characterization
of the optimal weight function.
\item \strong{Aggregation:} Per-observation votes from the Rashomon set
are combined (by default, majority vote) to produce the final weight
vector \code{w_opt}.
\item \strong{Characteristic tree:} A single summary decision tree is
fitted to the aggregated \code{w_opt} assignments, providing a concise,
interpretable description of the weight function.
}
}

\section{Generalizability mode}{

When \code{generalizability_path = TRUE}, ROOT implements the methodology
of Parikh et al. (2025) for characterizing underrepresented subgroups in
trial-to-target generalizability analyses. In this mode:

\itemize{
\item \code{data} must contain columns \code{Y} (outcome), \code{Tr}
(treatment, 0/1), and \code{S} (sample indicator, 1 = trial, 0 =
target).
\item ROOT internally computes transportability scores based on
inverse-probability weighting (IPW), estimates the selection model
\eqn{P(S = 1 \mid X)}, and constructs Horvitz-Thompson-style
influence scores.
\item The default objective minimizes the variance of the weighted
target average treatment effect (WTATE) estimator. This objective
accounts for both the selection odds (trial participation
probability) and treatment effect heterogeneity, so that subgroups
are flagged as underrepresented only when they both lack trial
representation and exhibit effect modification.
\item The output includes the unweighted sample average treatment
effect (SATE) and the WTATE with standard errors.
}

See \code{\link{characterizing_underrep}} for a higher-level wrapper that
additionally produces a leaf-level summary table, and
\code{vignette("generalizability_path_example")} for a worked example.
}

\section{General optimization mode}{

When \code{generalizability_path = FALSE}, ROOT operates as a general
functional optimizer. The user supplies any \code{data.frame} and
(optionally) a custom \code{global_objective_fn}. If no objective is
supplied, ROOT uses a default variance-based loss operating on the
\code{vsq} column (per-unit variance proxy). See
\code{vignette("optimization_path_example")} for an example.
}

\examples{
\dontrun{
# --- Generalizability mode ---
data(diabetes_data, package = "ROOT")
root_fit <- ROOT(
  data                  = diabetes_data,
  generalizability_path = TRUE,
  num_trees             = 20,
  top_k_trees           = TRUE,
  k                     = 10,
  seed                  = 123
)
summary(root_fit)
plot(root_fit)

# --- General optimization mode (custom objective) ---
my_objective <- function(D) {
  w <- D$w
  if (sum(w) == 0) return(Inf)
  sqrt(sum(w * D$vsq) / sum(w)^2)
}
opt_fit <- ROOT(
  data                = my_data,
  global_objective_fn = my_objective,
  num_trees           = 20,
  seed                = 42
)
}
}
\references{
Parikh H, Ross RK, Stuart E, Rudolph KE (2025).
"Who Are We Missing?: A Principled Approach to Characterizing the
Underrepresented Population."
\emph{Journal of the American Statistical Association}.
\doi{10.1080/01621459.2025.2495319}
}
\seealso{
\code{\link{characterizing_underrep}} for a higher-level wrapper with
leaf-summary output; \code{vignette("generalizability_path_example")} for
the generalizability workflow;
\code{vignette("optimization_path_example")} for general optimization.
}
